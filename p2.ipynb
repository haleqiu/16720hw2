{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b622c290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d0576b1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "21adcbd45e8054cae0af933c776688d9",
     "grade": false,
     "grade_id": "cell-a24dc0c293818e90",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<img align=\"center\" src=\"figures/course.png\" width=\"800\">\n",
    "\n",
    "#                                    16720 (B) Bag of Visual Words - Assignment 2\n",
    "\n",
    "     Instructor: Kris Kitani                      TAs:Paritosh (Lead), Rawal, Yan, Zen, Wen-Hsuan, Qichen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946f27c7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "beb60fd0b53579242373d566d9eac74a",
     "grade": false,
     "grade_id": "cell-5bf9175efa654233",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Building a Recognition System\n",
    "We have formed a convenient way to represent images for recognition. We will now produce a basic recognition system with spatial pyramid matching. The goal of the system is presented in Fig. 1:\n",
    "given an image, classify (\\ie recognize/name) the scene where the image was taken. \n",
    "\n",
    "Traditional classification problems follow two phases: training and testing.\n",
    "At training time, the computer is given a pile of formatted data (\\ie, a collection\n",
    "of feature vectors) with corresponding labels (\\eg, ``desert``, ``kitchen``) and\n",
    "then builds a model of how the data relates to the labels:\n",
    "``if green, then kitchen``. At test time, the computer takes features and uses these rules to infer the label:\n",
    "\\eg, ``this is green, so therefore it is kitchen``.\n",
    "\n",
    "In this assignment, we will use the simplest classification model: nearest neighbor.\n",
    "At test time, we will simply look at the query's nearest neighbor in the training set\n",
    "and transfer that label. In this example, you will be looking\n",
    "at the query image and \n",
    "looking up its nearest neighbor in a collection of training images whose labels are already known. This approach works\n",
    "surprisingly well given a huge amount of data, \\eg, a very cool graphics applications from [1]. \n",
    "\n",
    "The key components of any nearest-neighbor system are: \n",
    "* $features$ (how do you represent your instances?)\n",
    "* $similarity$ (how do you compare instances in the feature space?)\n",
    "\n",
    "You will implement both in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97b6eca9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b058da0e9e9aab7bf35282d082f3bba3",
     "grade": false,
     "grade_id": "cell-24621a038b7f3018",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "import numpy as np\n",
    "import skimage\n",
    "import multiprocessing\n",
    "import threading\n",
    "import queue\n",
    "import os,time\n",
    "import math\n",
    "from ipynb.fs.defs.p1 import get_visual_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6390ab",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "42b93f2fd34d13d0deb697d261a757d5",
     "grade": false,
     "grade_id": "cell-7c3a5b2d9b8ce219",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## For Autograding P2, ensure uploading `trained_system.npz`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c601467d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2982222ee35569544c9b82765433de7d",
     "grade": false,
     "grade_id": "cell-abf0004ccc069506",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q2.1 (10 Points -> 5 Autograder + 5 WriteUp)\n",
    "We will first represent an image with a bag of words approach. In each image, we simply look at\n",
    "how often each word appears.\n",
    "\n",
    "Write the function\n",
    "```\n",
    "            def get_feature_from_wordmap(wordmap,dict_size):\n",
    "```\n",
    "that extracts the histogram (Look into ``numpy.histogram()``) of visual words within the given image\n",
    "(\\ie, the bag of visual words). \n",
    "As inputs, the function will take:\n",
    "\n",
    "* $wordmap$ is a $H$ $\\times$ $W$ image containing the IDs of the visual words\n",
    "* $dict\\_size$ is the maximum visual word ID (\\ie, the number of visual words, the dictionary size). Notice that your histogram should have $dict\\_size$ bins, corresponding to how often that each word occurs. \n",
    "\n",
    "As output, the function will return $hist$, a $dict\\_size$ histogram that is $L_1$ normalized, (i.e., the sum equals $1$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4550429e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1254416f040933e0666d2047ffa467e5",
     "grade": false,
     "grade_id": "cell-8e9543ec71e3c588",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_feature_from_wordmap(wordmap, dict_size):\n",
    "    '''\n",
    "    Compute histogram of visual words.\n",
    "\n",
    "    [input]\n",
    "    * wordmap: numpy.ndarray of shape (H,W)\n",
    "    * dict_size: dictionary size K\n",
    "\n",
    "    [output]\n",
    "    * hist: numpy.ndarray of shape (K)\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    HINTS:\n",
    "    (1) We can use np.histogram with flattened wordmap\n",
    "    '''\n",
    "    # ----- TODO -----\n",
    "    # YOUR CODE HERE\n",
    "    hist = np.histogram(wordmap.reshape(-1),range(dict_size + 1),density=True)\n",
    "    return hist[0]#*np.diff(hist[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "cea61380",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_1 = \"./data/aquarium/sun_axahqdyqpausckwh.jpg\"\n",
    "image = skimage.io.imread(test_image_1)\n",
    "dic = np.load('dictionary.npy')\n",
    "vis = get_visual_words(image, dic)\n",
    "hist_np = get_feature_from_wordmap(vis,dic.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a90dae",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "68d581bfafe37f38106fb00b90fde05b",
     "grade": false,
     "grade_id": "cell-e84e8b8406598ee0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<font color=\"blue\">**For 5 Images, load visual word maps, visualize their histogram, and include it in the write up.**</font> This will help you verifying that your function is working correctly before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a0e29f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "402beda322b6595dc546197d4236ad5f",
     "grade": false,
     "grade_id": "cell-420ecef2e3112019",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Multi-resolution: Spatial Pyramid Matching\n",
    "\n",
    "Bag of words is simple and efficient, but it discards information about the spatial structure of the image and this information is often valuable. One way to alleviate this issue is to use spatial pyramid matching [2]. The general idea is to divide the image into a small number of cells, and concatenate the histogram of each of these cells to the histogram of the original image, with a suitable weight. \n",
    "\n",
    "Here we will implement a popular scheme that chops the image into $2^l\\times2^l$ cells where $l$ is the layer number. We treat each cell as a small image and count how often each visual word appears. This results in a histogram for every single cell in every layer. Finally to represent the entire image, we concatenate all the histograms together. If there are $L+1$ layers and $K$ visual words, the resulting vector has dimensionality $K\\sum_{l=0}^L{4^l} = K\\left(4^{(L+1)}-1\\right)/3$.\n",
    "\n",
    "Now comes the weighting scheme. Note that when concatenating all the histograms, histograms from different levels are assigned different weights. Typically (and in the original work [2]), features from layer $l$ gets half the weight of features from layer $l+1$, with the exception of layer 0, which is assigned a weight equal to layer 1. A popular choice is for layer $0$ and layer $1$ the weight is set to $2^{-L}$, and for the rest it is set to $2^{l-L-1}$ (\\eg, in a three layer spatial pyramid, $L=2$ and weights are set to $1/4$, $1/4$ and $1/2$ for layer 0, 1 and 2 respectively, see Fig. 7). Take level 2 as an example, there will be 16 histograms in total, each has a norm equal to one. You should concatenate these histograms, normalize this layer (multiply the concatenated vector by 1/16), and apply the 1/2 layer weight of level 2 on top of that. Note that following this operation, concatenating the weighted features of each layer will result in a final vector of norm equal to 1.\n",
    "\n",
    "<img align=\"center\" src=\"figures/spm.jpg\" width=\"800\">\n",
    "<figcaption align = \"center\"><b>Figure 7. Spatial Pyramid Matching: From [2]. Toy example of a pyramid for L = 2. The image has three visual words, indicated by circles, diamonds, and crosses. We subdivide the image at three different levels of resolution. For each level of resolution and each channel, we count the features that fall in each spatial bin. Finally, weight each spatial histogram.}</b></figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12637a1f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6e7ceda3fd88f1fee5ddf1ff7b6f928c",
     "grade": false,
     "grade_id": "cell-37520ce1e3b2405a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q2.2.1 (15 Points Autograder)\n",
    "\n",
    "Create a function that form a multi-resolution representation of the given image.\n",
    "```\n",
    "            def get_feature_from_wordmap_SPM(wordmap,layer_num,dict_size):\n",
    "```\n",
    "As inputs, the function will take:\n",
    "\n",
    "* **layer_num** the number of layers in the spatial pyramid, \\ie, $L+1$\n",
    "* **wordmap** is a $H$ $\\times$ $W$ image containing the IDs (\\ie index) of the visual words\n",
    "* **dict_size** is the maximum visual word ID (\\ie, the number of visual words, the dictionary size)\n",
    "\n",
    "As output, the function will return hist_all, a vector that is $L_1$ normalized. **Please use a 3-layer spatial pyramid ($L=2$) for all the following recognition tasks.**\n",
    "\n",
    "One small hint for efficiency: a lot of computation can be saved if you first compute the histograms of the _finest layer_, because the histograms of coarser layers can then be aggregated from finer ones. Make sure you normalize the histogram after aggregation.\n",
    "\n",
    "**Note for Autograder :** Ensure that final $hist\\_all$ (the output of `get_feature_from_wordmap_SPM`) has histogram features arranged from Loweset Level (global features) to Highest Level (finest features). Example: the output array should **first contain the histogram for Level 0, followed by Level 1, and then Level 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "405c743a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ea2abb5b089e2e2a54f5eacfcb47505",
     "grade": false,
     "grade_id": "cell-1ce0fa9511c64ad7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_feature_from_wordmap_SPM(wordmap, layer_num, dict_size):\n",
    "    '''\n",
    "    Compute histogram of visual words using spatial pyramid matching.\n",
    "\n",
    "    [input]\n",
    "    * wordmap: numpy.ndarray of shape (H,W)\n",
    "    * layer_num: number of spatial pyramid layers\n",
    "    * dict_size: dictionary size K\n",
    "\n",
    "    [output]\n",
    "    * hist_all: numpy.ndarray of shape (K*(4^layer_num-1)/3)\n",
    "    '''\n",
    "    '''\n",
    "    HINTS:\n",
    "    (1) Take care of Weights \n",
    "    (2) Try to build the pyramid in Bottom Up Manner\n",
    "    (3) the output array should first contain the histogram for Level 0 (top most level) , followed by Level 1, and then Level 2.\n",
    "    '''\n",
    "    # ----- TODO -----\n",
    "    h, w = wordmap.shape\n",
    "    L = layer_num - 1\n",
    "    patch_width = math.floor(w / (2**L))\n",
    "    patch_height = math.floor(h / (2**L))\n",
    "    \n",
    "    '''\n",
    "    HINTS:\n",
    "    1.> create an array of size (dict_size, (4**(L + 1) -1)/3) )\n",
    "    2.> pre-compute the starts, ends and weights for the SPM layers L \n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    hist_all = np.zeros((dict_size, int((4**(L + 1) -1)/3)))\n",
    "    weights = [s* (1/4**l) for l, s in zip(range(layer_num),[1/4,1/4,1/2])]\n",
    "    \n",
    "    '''\n",
    "    HINTS:\n",
    "    1.> Loop over the layers from L to 0\n",
    "    2.> Handle the base case (Layer L) separately and then build over that\n",
    "    3.> Normalize each histogram separately and also normalize the final histogram\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    counter = 0\n",
    "    for l,weight in zip(range(layer_num),weights):\n",
    "        for i in range(4**l):\n",
    "            patch_width = math.floor(w / (2**l))\n",
    "            patch_height = math.floor(h / (2**l))\n",
    "            _w = i%(2**l)\n",
    "            _h = int(i/(2**l))\n",
    "            patch = wordmap[_w*patch_width:_w*patch_width+patch_width,_h*patch_height:_h*patch_height+patch_height]\n",
    "            hist_patch = get_feature_from_wordmap(patch, dict_size)\n",
    "            hist_all[:,counter] = hist_patch*weight\n",
    "            counter+=1\n",
    "            np.save('temp_hist',hist_all)\n",
    "        \n",
    "    return hist_all.reshape(-1)\n",
    "# test_image_1 = \"./data/aquarium/sun_axahqdyqpausckwh.jpg\"\n",
    "# image = skimage.io.imread(test_image_1)\n",
    "# dic = np.load('dictionary.npy')\n",
    "# vis = get_visual_words(image, dic)\n",
    "# hist_all_out = get_feature_from_wordmap_SPM(vis,3,dic.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "ba4f73a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_1 = \"./data/aquarium/sun_axahqdyqpausckwh.jpg\"\n",
    "image = skimage.io.imread(test_image_1)\n",
    "dic = np.load('dictionary.npy')\n",
    "vis = get_visual_words(image, dic)\n",
    "hist_all_out = get_feature_from_wordmap_SPM(vis,3,dic.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "5c086a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999999"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(hist_all_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440e8b67",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "04032ab193e9ca78f8fe4c9b8ee94b5e",
     "grade": false,
     "grade_id": "cell-66d373829f0267b8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.3 Comparing images\n",
    "\n",
    "\n",
    "We will also need a way of comparing images to find the _nearest_ instance in the training data. In this assignment, we'll use the histogram intersection similarity. The histogram intersection similarity between two histograms is the sum of the minimum value of each corresponding bins.\n",
    "\n",
    "Note that since this is a similarity, you want the $largest$ value to find the _nearest_ instance.\n",
    "\n",
    "#### Q2.3.1 (10 Points Autograder)\n",
    "Create the function\n",
    "```\n",
    "                def distance_to_set(word_hist,histograms):\n",
    "```\n",
    "where $word\\_hist$ is a $K\\left(4^{(L+1)}-1\\right)/3$ vector and $histograms$ is a $T \\times K\\left(4^{(L+1)}-1\\right)/3$ matrix containing $T$ features from $T$ training samples concatenated along the rows. This function returns the histogram intersection similarity between $word\\_hist$ and each training sample as a vector of length $T$. Since this is called every time you want to look up a classification, you want this to be fast, so doing a for-loop over tens of thousands of histograms is a very bad idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "33b4acca",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "65968a4a820824572f5715b4776866b4",
     "grade": false,
     "grade_id": "cell-465ba3be1342934c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def distance_to_set(word_hist, histograms):\n",
    "    '''\n",
    "    Compute similarity between a histogram of visual words with all training image histograms.\n",
    "\n",
    "    [input]\n",
    "    * word_hist: numpy.ndarray of shape (K)\n",
    "    * histograms: numpy.ndarray of shape (N,K)\n",
    "\n",
    "    [output]\n",
    "    * sim: numpy.ndarray of shape (N)\n",
    "    '''\n",
    "    '''\n",
    "    HINTS:\n",
    "    (1) Consider A = [0.1,0.4,0.5] and B = [[0.2,0.3,0.5],[0.8,0.1,0.1]] then \\\n",
    "        similarity between element A and set B could be represented as [[0.1,0.3,0.5],[0.1,0.1,0.1]]   \n",
    "    '''\n",
    "    # ----- TODO -----\n",
    "    # YOUR CODE HERE\n",
    "    sim = np.minimum(histograms,word_hist).sum(axis=1)\n",
    "    \n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c043d49f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9, 0.3])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_to_set(np.array([0.1,0.4,0.5]),np.array([[0.2,0.3,0.5],[0.8,0.1,0.1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361f3d21",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b6e3eb69fe9118a07d7c0854176fb7bb",
     "grade": false,
     "grade_id": "cell-5d13fb8f4aab2c9d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q2.4 Building a Model of the Visual World\n",
    "\n",
    "Now that we've obtained a representation for each image, and defined a similarity measure to compare two spatial pyramids, we want to put everything up to now together.\n",
    "\n",
    "You will need to load the training file names from `data/train_data.npz` and the filter bank and visual word dictionary from `dictionary.npy`.\n",
    "You will save everything to a `.npz` numpy-formated (use `np.savez`) file named `trained_system.npz`. Included will be:\n",
    "\n",
    "\n",
    "1. $dictionary$: your visual word dictionary.\n",
    "2. $features$: a $N \\times  K\\left(4^{(L+1)}-1\\right)/3$ matrix containing all of the histograms of the $N$ training images in the data set. A dictionary with $150$ words will make a ``train_features`` matrix of size $1000 \\times 3150$.\n",
    "3. $labels$: an $N$ vector containing the labels of each of the images. ( ``features[i]`` will correspond to label ``labels[i]``).\n",
    "4. $SPM\\_layer\\_num$: the number of spatial pyramid layers you used to extract the features for the training images.\n",
    "\n",
    "We have provided you with the names of the training images in ``data/train_data.npz``.\n",
    "You want to use the dictionary entry ``image_names`` for training.\n",
    "You are also provided the names of the test images in ``data/test_data.npz``, which is structured in the same way as the training data; however, _you cannot use the testing images for training._\n",
    "\n",
    "If it's any helpful, the below table lists the class names that correspond to the label indices:\n",
    "\n",
    "| 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |\n",
    "| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\n",
    "|aquarium | park | desert | highway | kitchen | laundromat | waterfall | windmill|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4029641",
   "metadata": {},
   "source": [
    "#### Q2.4.1 (15 Points Autograder)\n",
    "Implement the function\n",
    "```\n",
    "                def build_recognition_system():\n",
    "```\n",
    "that produces ``trained_system.npz``.\n",
    "\n",
    "Implement \n",
    "```\n",
    "                def get_image_feature(file_path,dictionary,layer_num,K):\n",
    "```\n",
    "that load image, extract word map from the image, compute SPM feature and return the computed feature. Use this function in your ``build_recognition_system()``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "0e9faf82",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "70bf1a70ee97f9b567d81581a8c94d8d",
     "grade": false,
     "grade_id": "cell-bd6c5a8aa3b427b9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_image_feature(file_path, dictionary, layer_num, K):\n",
    "    '''\n",
    "    Extracts the spatial pyramid matching feature.\n",
    "\n",
    "    [input]\n",
    "    * file_path: path of image file to read\n",
    "    * dictionary: numpy.ndarray of shape (K,3F)\n",
    "    * layer_num: number of spatial pyramid layers\n",
    "    * K: number of clusters for the word maps\n",
    "\n",
    "    [output]\n",
    "    * feature: numpy.ndarray of shape (K*(4^layer_num-1)/3)\n",
    "    '''\n",
    "    # ----- TODO -----\n",
    "    # YOUR CODE HERE\n",
    "    image = skimage.io.imread(file_path)\n",
    "    wordmap = get_visual_words(image,dictionary)\n",
    "    feature = get_feature_from_wordmap_SPM(wordmap, layer_num, K)\n",
    "    return [file_path, feature]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "c787bb28",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "baf93b4c9286323f36aca1d82a1711a9",
     "grade": false,
     "grade_id": "cell-5465a0a46dfde2c5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def build_recognition_system(num_workers=2):\n",
    "    '''\n",
    "    Creates a trained recognition system by generating training features from all training images.\n",
    "\n",
    "    [input]\n",
    "    * num_workers: number of workers to process in parallel\n",
    "\n",
    "    [saved]\n",
    "    * features: numpy.ndarray of shape (N,M)\n",
    "    * labels: numpy.ndarray of shape (N)\n",
    "    * dictionary: numpy.ndarray of shape (K,3F)\n",
    "    * SPM_layer_num: number of spatial pyramid layers\n",
    "    '''\n",
    "\n",
    "    train_data = np.load(\"./data/train_data.npz\")\n",
    "    dictionary = np.load(\"dictionary.npy\")\n",
    "    # ----- TODO -----\n",
    "    # YOUR CODE HERE\n",
    "    K = dictionary.shape[0]\n",
    "    SPM_layer_num = 3\n",
    "    image_name_list = train_data['files']\n",
    "    \n",
    "    labels = train_data['labels']\n",
    "    ordered_features = []\n",
    "    for file_path in image_name_list:\n",
    "        _,feature = get_image_feature(os.path.join(\"data\",file_path), dictionary, SPM_layer_num, K)\n",
    "        ordered_features.append(feature)\n",
    "    ordered_features = np.array(ordered_features)\n",
    "    np.savez('trained_system.npz', features=ordered_features,\n",
    "                                    labels=labels,\n",
    "                                    dictionary=dictionary,\n",
    "                                    SPM_layer_num=SPM_layer_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "5de93ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/hw/lib/python3.8/site-packages/numpy/lib/histograms.py:905: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return n/db/n.sum(), bin_edges\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yh/q66kd_1s4ll210yb3kfz417m0000gn/T/ipykernel_13415/2965108868.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbuild_recognition_system\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/yh/q66kd_1s4ll210yb3kfz417m0000gn/T/ipykernel_13415/3900459182.py\u001b[0m in \u001b[0;36mbuild_recognition_system\u001b[0;34m(num_workers)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mordered_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage_name_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_image_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSPM_layer_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mordered_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mordered_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mordered_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/yh/q66kd_1s4ll210yb3kfz417m0000gn/T/ipykernel_13415/18442554.py\u001b[0m in \u001b[0;36mget_image_feature\u001b[0;34m(file_path, dictionary, layer_num, K)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mwordmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_visual_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_feature_from_wordmap_SPM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/16720/hw2/p1.ipynb\u001b[0m in \u001b[0;36mget_visual_words\u001b[0;34m(image, dictionary)\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;34m\"        return points_of_interest\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[0;34m\"```\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m     \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m     \u001b[0;34m\"This function takes the input image, $\\\\alpha$ as number of points we want to choose, and k the sensitivity factor. The input image may either be a color or grayscale image, but the following operations should be done on the grayscale representation of the image (feel free to use functions from ``cv2``, ``skimage``, etc. for the conversion). For each pixel, you need to compute the covariance matrix:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m     \u001b[0;34m\"$$ \\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/16720/hw2/p1.ipynb\u001b[0m in \u001b[0;36mextract_filter_responses\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m    153\u001b[0m     }\n\u001b[1;32m    154\u001b[0m    },\n\u001b[0;32m--> 155\u001b[0;31m    \u001b[0;34m\"outputs\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m    \"source\": [\n\u001b[1;32m    157\u001b[0m     \u001b[0;34m\"# Do Not Modify\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/hw/lib/python3.8/site-packages/scipy/ndimage/filters.py\u001b[0m in \u001b[0;36mgaussian_filter\u001b[0;34m(input, sigma, order, output, mode, cval, truncate)\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m             gaussian_filter1d(input, sigma, axis, order, output,\n\u001b[0m\u001b[1;32m    299\u001b[0m                               mode, cval, truncate)\n\u001b[1;32m    300\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/hw/lib/python3.8/site-packages/scipy/ndimage/filters.py\u001b[0m in \u001b[0;36mgaussian_filter1d\u001b[0;34m(input, sigma, axis, order, output, mode, cval, truncate)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;31m# Since we are calling correlate, not convolve, revert the kernel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_gaussian_kernel1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcorrelate1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/hw/lib/python3.8/site-packages/scipy/ndimage/filters.py\u001b[0m in \u001b[0;36mcorrelate1d\u001b[0;34m(input, weights, axis, output, mode, cval, origin)\u001b[0m\n\u001b[1;32m     92\u001b[0m                          '(len(weights)-1) // 2')\n\u001b[1;32m     93\u001b[0m     \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ni_support\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_mode_to_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     _nd_image.correlate1d(input, weights, axis, output, mode, cval,\n\u001b[0m\u001b[1;32m     95\u001b[0m                           origin)\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "build_recognition_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8e0ae4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "db3f9591ac0e2778ca5ef665c6227fa6",
     "grade": false,
     "grade_id": "cell-63d0143d4e3d0c6a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### References\n",
    "\n",
    "[1]  James Hays and Alexei A Efros. Scene completion using millions of photographs.ACM Transactions onGraphics (SIGGRAPH 2007), 26(3), 2007.\n",
    "\n",
    "[2]  S. Lazebnik, C. Schmid, and J. Ponce. Beyond bags of features: Spatial pyramid matching for recogniz-ing natural scene categories.  InComputer Vision and Pattern Recognition (CVPR), 2006 IEEE Conferenceon, volume 2, pages 2169–2178, 2006.\n",
    "\n",
    "[3]  Jian xiong Xiao, J. Hays, K. Ehinger, A. Oliva, and A. Torralba. Sun database: Large-scale scene recogni-tion from abbey to zoo.2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition,pages 3485–3492, 2010.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b060ab",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "35ee79b5e5fccba897d0d2d9dc949c9d",
     "grade": true,
     "grade_id": "q_2_1_1",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b94c22",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ebeebf6212f93c0c3223f32582217689",
     "grade": true,
     "grade_id": "q_2_2_1",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12284c00",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2e93e34336483b7c51eb11230b99a89f",
     "grade": true,
     "grade_id": "q_2_3_1",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26109557",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "870554a8fedbc04b85337e1ded2ef277",
     "grade": true,
     "grade_id": "q_2_4_1",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
