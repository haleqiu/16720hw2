{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b3b32c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "586427e7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dfd5012891db3ad5af3cde5e2522aa02",
     "grade": false,
     "grade_id": "cell-6067354e526d6417",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<img align=\"center\" src=\"figures/course.png\" width=\"800\">\n",
    "\n",
    "#                                    16720 (B) Bag of Visual Words - Assignment 2\n",
    "\n",
    "     Instructor: Kris Kitani                          TAs:Paritosh (Lead), Rawal, Yan, Zen, Wen-Hsuan, Qichen\n",
    "\n",
    "\n",
    "Perhaps the most common problem in computer vision is classification. Given an image that comes from a few pre-defined categories, can you determine which category it belongs to? For this assignment, you will be developing a system for scene classification. You will be experimenting on a subset of the SUN Image database [3] consisting of eight scene categories and build an end to end system that will, given a new scene image, determine which type of scene it is, as shown in Figure 1.\n",
    "\n",
    "|![Teaser](figures/teaser/teaser.png)|\n",
    "|:--:|\n",
    "|Fig.1 **Scene Classification**: Given an image, can a computer program determine where it was taken? In this homework, you will build a representation based on bags of visual words and use spatial pyramid matching for classifying the scene categories.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6c868f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "04075f5f5b547d9b8c8f38316d51a1b2",
     "grade": false,
     "grade_id": "cell-5ba45fd8baf2a37f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "This assignment is based on an approach for document classification called **Bag of Words**. It represents a document as a vector or histogram of counts for each word that occurs in the document, as shown in Figure 2. The hope is that different documents in the same class will have a similar collection and distribution of words, and that when we see a new document, we can find out which class it belongs to by comparing it to the word distribution already seen in that class. This approach has been very successful in Natural Language Processing, which is surprising due to its relative simplicity (we are throwing away all the sequential relationships and representing each document as mere counts of words it contains!).\n",
    "\n",
    "We will be taking inspiration from this approach and apply it to image classification: imagine each image as a document, and we want to classify each document as a scene. \n",
    "\n",
    "|![bagofwords](figures/bagofwords.png)|\n",
    "|:--:|\n",
    "|Fig.2 Bag of words representation of a text document|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a17b10",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "00578c4cef481e256f457f4576f4053f",
     "grade": false,
     "grade_id": "cell-726549ea92913ef1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For this assignment ensure that you use the following versions of libraries\n",
    "* nbimporter==0.3.4                                \n",
    "* torch==1.8.1\n",
    "* torchvision==0.9.1\n",
    "* numpy==1.19.5\n",
    "* matplotlib==3.3.4\n",
    "* opencv-python==4.5.3.56\n",
    "* scipy==1.5.2\n",
    "* scikit-image==0.17.2\n",
    "* scikit-learn==0.24.2\n",
    "* ipynb==0.5.1\n",
    "we also provide a requirements.txt. Students can use `pip install -r requirements.txt`.\n",
    "\n",
    "## For Autograding P1, ensure to upload `dictionary.npy`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfd6bc3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fb5846835396ecd99bcd7fc31531880b",
     "grade": false,
     "grade_id": "cell-06fd57110dcee345",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "This assignment has 4 major sections:\n",
    "\n",
    "* **P1**: build a dictionary of **visual words** from training data. In this section, you will use Harris corner detector to pick interest points, extract visual words from interest points on images, form a visual dictionary, and represent each image as a vector of visual words. \n",
    "* **P2**: build a recognition system using visual word dictionary and training images. In this section, you will use the visual words extracted from the previous section and a technique called __Spatial Pyramid Matching__ to extract visual features from images. You will use these features and ground truth labels to build a simple recognition system.\n",
    "* **P3**: evaluate the recognition system on test images. In this section, you and evaluate your system and classify a given image to 8 types of scenes. \n",
    "* **P4**: Explore an alternative to BoW -- Deep Learning Features, and compare results between the two approaches. \n",
    "\n",
    "\n",
    "An illustrative overview of the homework is shown in Figure. 3.\n",
    "\n",
    "|![Overview](figures/overview.png)|\n",
    "|:--:|\n",
    "|Fig. 3: An overview of the bags-of-words approach to be implemented in the homework. Given the training set of images, the visual features of the images are extracted. In our case, we will use the filter responses of the pre-defined filter bank as the visual features. The visual words, \\ie dictionary, are built as the centers of clusterings of the visual features. During recognition, the image is first represented as a vector of visual words. Then the comparison between images is realized in the visual-word vector space. Finally, we will build a scene recognition system that classifies the given image into $8$ types of scene|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2bbe24e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c1331090dda98ced043c10b9cebfbb1b",
     "grade": false,
     "grade_id": "cell-7713183c2edb06a4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-37898f0554e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Do Not Modify\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnbimporter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplay_filter_responses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/16720/hw2/util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# Do Not Modify\n",
    "import nbimporter\n",
    "from util import display_filter_responses\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import scipy.ndimage\n",
    "import skimage\n",
    "import sklearn.cluster\n",
    "import scipy.spatial.distance\n",
    "import os, time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "from skimage import io\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def plot_harris_points(image,points):\n",
    "    fig = plt.figure(1)\n",
    "    for x,y in zip(points[0],points[1]):\n",
    "        plt.plot(y,x,marker='v')\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4461eb7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e90a4b26915bd33b9442cd79de3cc1a3",
     "grade": false,
     "grade_id": "cell-386e0d6f3147d142",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q1.1 Extracting Filter Responses\n",
    "\n",
    "To extract image features as visual words, we will rely on using a multi-scale filter bank. We want to run a filter bank on an image by convolving each filter in the bank with the image and concatenating all the responses into a vector for each pixel.\n",
    "\n",
    "In our case, we will be using $20$ filters consisting of $4$ types of filters in $5$ scales.\n",
    "The filters are: (1) Gaussian, (2) Laplacian of Gaussian, (3) derivative of Gaussian in the $x$ direction, and (4) derivative of Gaussian in the $y$ direction.\n",
    "\n",
    "Students can refer to ```scipy.ndimage.gaussian_filter``` and ```scipy.ndimage.gaussian_laplace``` for generating the filters. \n",
    "\n",
    "<img align=\"center\" src=\"figures/filters_image.png\" width=\"500\">\n",
    "<figcaption align = \"center\"><b>Figure 4. The provided multi-scale filter bank</b></figcaption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c21395c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'scipy' has no attribute 'ndimage'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-75ec965d6e31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgaussian_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'scipy' has no attribute 'ndimage'"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "scipy.ndimage.gaussian_filter(sigma = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370289b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3bf8cbf2",
   "metadata": {},
   "source": [
    "#### Q1.1.2 (10 Points -> 5 from Autograder + 5 from WriteUp)\n",
    "\n",
    "To implement, loop through the filters and the scales to extract responses. Since color images have $3$ channels, you are going to have a total of $3F$ filter responses per pixel if the filter bank is of size $F$. Note that in the given dataset, there are some gray-scale images. For those gray-scale images, you can simply stack them into three channels using the command ``np.stack``. Then output the result as a $3F$ channel image. Complete the function \n",
    "```\n",
    "                        def extract_filter_responses(image):\n",
    "                            return filter_responses\n",
    "```\n",
    "and return the responses as ``filter_responses``.\n",
    "\n",
    "We have provided you with a template code with detailed instructions in it. You would be required to input a 3-channel RGB or gray-scale image and filter bank to get the responses of the filters on the image.\n",
    "\n",
    "Remember to check the input argument $image$ to make sure it is a floating point type with range 0~1, and convert it if necessary. Be sure to check the number of input image channels and convert it to 3-channel if it is not. Before applying the filters, use the function ``skimage.color.rgb2lab()`` to convert your image into the $Lab$ color space, which was designed to more effectively quantify color differences with respect to human perception (See [here](https://en.wikipedia.org/wiki/CIELAB_color_space) for more information.). Notice that after converting the image to $Lab$ color space, it will no longer in the range of [0,1], this is expected. If $image$ is an $M \\times N \\times 3$ matrix, then ``filter_responses`` should be a matrix of size $M \\times N \\times 3F$. Make sure your convolution function call handles image padding along the edges sensibly (by passing in the right options). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e42201",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "162aafcb40d71ad217bb420079e4d488",
     "grade": false,
     "grade_id": "cell-130be47ec3492dc3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def extract_filter_responses(image):\n",
    "    '''\n",
    "    Extracts the filter responses for the given image.\n",
    "\n",
    "    [input]\n",
    "    * image: numpy.ndarray of shape (H, W) or (H, W, 3)\n",
    "\n",
    "    [output]\n",
    "    * filter_responses: numpy.ndarray of shape (H, W, 3F)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    if(len(image.shape) == 2):\n",
    "        image = np.stack((image, image, image), axis=-1)\n",
    "\n",
    "    if(image.shape == 3 and image.shape[2] == 1):\n",
    "        image = np.concatenate((image, image, image), axis=-1)\n",
    "\n",
    "    if(image.shape[2] == 4):\n",
    "        image = image[:, :, 0:3]\n",
    "\n",
    "    image = skimage.color.rgb2lab(image)\n",
    "\n",
    "    filter_responses = []\n",
    "    '''\n",
    "    HINTS: \n",
    "    1.> Iterate through the scales (5) which can be 1, 2, 4, 8, 8√2\n",
    "    2.> use scipy.ndimage.gaussian_* to create filters\n",
    "    3.> Iterate over each of the three channels independently\n",
    "    4.> stack the filters together to (H, W,3F) dim\n",
    "    '''\n",
    "    # ----- TODO -----\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return filter_responses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dd0774",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aff28b9a96f0e0722c8d97dff8a8bf8b",
     "grade": true,
     "grade_id": "q_1_1_2",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def unittest_extract_filter_response():\n",
    "    train_data = np.load(\"./data/train_data.npz\")\n",
    "    image_names = train_data['files']\n",
    "    for t in range(0,5):\n",
    "        idx = np.random.randint(0,len(image_names)-1)\n",
    "        test_image_1 = \"./data/\" + image_names[idx]\n",
    "#         test_image_1 = \"./data/aquarium/sun_axahqdyqpausckwh.jpg\"\n",
    "        image = io.imread(test_image_1)\n",
    "        image = image.astype('float')/255\n",
    "\n",
    "        h,w = image.shape[0],image.shape[1]\n",
    "        num_channels = 60\n",
    "        \n",
    "        std_filter_response = extract_filter_responses(image)\n",
    "        try:\n",
    "            assert std_filter_response.shape[0] == h\n",
    "        except:\n",
    "            raise AssertionError('Test Case {}: failed  Wrong Height'.format(idx+1))\n",
    "        try:\n",
    "            assert std_filter_response.shape[1] == w\n",
    "        except:\n",
    "            raise AssertionError('Test Case {}: failed  Wrong Width'.format(idx+1))\n",
    "        try:\n",
    "            assert std_filter_response.shape[2] == num_channels\n",
    "        except:\n",
    "            raise AssertionError('Test Case {}: failed  Wrong Channels'.format(idx+1))\n",
    "        \n",
    "        \n",
    "        image = image[:,:,0]\n",
    "        try:\n",
    "            std_filter_response = extract_filter_responses(image)\n",
    "        except:\n",
    "            raise AssertionError('Test Case {}: failed  Cannot Handle Different Input Format'.format(idx+1))\n",
    "    \n",
    "unittest_extract_filter_response()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e420c67a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bd2dd58e554c58a4bfe875c47f09e371",
     "grade": false,
     "grade_id": "cell-2ea575c69d4e3903",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<font color=\"blue\">Apply all 20 filters on the image $aquarium/sun\\_aztvjgubyrgvirup.jpg$, and visualize as a image collage (as shown in figure 5.). You can use the included helper function ``display_filter_responses()`` (which expects a list of filter responses with those of the Lab channels grouped together with shapeM×N×3) to create the collage. </font>\n",
    "\n",
    "<img align=\"center\" src=\"figures/example_filter.jpg\" width=\"500\">\n",
    "<img align=\"center\" src=\"figures/filters.png\" width=\"500\">\n",
    "<figcaption align = \"center\"><b>Figure 5. An input image and filter responses for all of the filters in the filter bank. (a) The input image (b) The filter responses of Lab image corresponding to the filters in Figure. 4</b></figcaption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521d6dca",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2e2fccfc8d86fe2490ec233347db6ec6",
     "grade": false,
     "grade_id": "cell-fd4555c8a294d42d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "path_img = \"./data/aquarium/sun_aztvjgubyrgvirup.jpg\"\n",
    "image = io.imread(path_img)\n",
    "image = image.astype('float')/255\n",
    "\n",
    "filter_responses = extract_filter_responses(image)\n",
    "display_filter_responses(filter_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597a71f3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "074b2ae6af2d93a742d8cd133c29d3d1",
     "grade": false,
     "grade_id": "cell-92bb2eb1c8295ec8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q1.2 Creating Visual Words\n",
    "\n",
    "You will now create a dictionary of visual words from the filter responses using `k-means`. After applying `k-means`, similar filter responses will be represented by the same visual word. You will use a dictionary with a fixed number of visual words. Instead of using all of the filter responses at each of the pixels from the given image **that can exceed the memory capacity of your computer**), you will use responses at $\\alpha$ interest points from each image, chosen using the Harris Corner Detector. If there are $T$ training images, then you should collect a matrix ``filter_responses`` over all the images that is $\\alpha * T \\times 3F$, where $F$ is the filter bank size (20 in our case).\n",
    "\n",
    "Then, to generate a visual words dictionary with $K$ words, you will cluster the responses with k-means using the function ``sklearn.cluster.KMeans`` as follows:\n",
    "```\n",
    "                kmeans = sklearn.cluster.KMeans(n_clusters=K).fit(filter_responses)\n",
    "                dictionary = kmeans.cluster_centers_ }\n",
    "```\n",
    "You can alternatively pass the `n_jobs` argument into the ``KMeans()`` object to utilize parallel computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e350db",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "15836a39cca4cb1efc32fd7028253698",
     "grade": false,
     "grade_id": "cell-df4d34fb2c2d2f17",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q1.2.1 (10 Points -> 5 Autograder + 5 WriteUp)\n",
    "\n",
    "As covered in class, the Harris Corner Detector is an effective way of selecting points of interest from images. This algorithm finds corners by building a covariance matrix of edge gradients within a region around a point in the image. The eigenvectors of this matrix point in the two directions of greatest change. If they are both large, then this indicates a corner. See class slides for more details.\n",
    "\n",
    "In this question, you will implement:\n",
    "```\n",
    "    get_harris_points(image, alpha, k)}:\n",
    "        return points_of_interest\n",
    "```\n",
    "\n",
    "This function takes the input image, $\\alpha$ as number of points we want to choose, and k the sensitivity factor. The input image may either be a color or grayscale image, but the following operations should be done on the grayscale representation of the image (feel free to use functions from ``cv2``, ``skimage``, etc. for the conversion). For each pixel, you need to compute the covariance matrix:\n",
    "$$ \n",
    "H=\\left[\\begin{array}{ll}\n",
    "\\sum_{p \\in P} I_{x x} & \\sum_{p \\in P} I_{x y} \\\\\n",
    "\\sum_{p \\in P} I_{y x} & \\sum_{p \\in P} I_{y y}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "where $I_{a b}=\\frac{\\partial I}{\\partial a} \\frac{\\partial I}{\\partial b}$, and p is the current window.. You can use a $3 \\times 3$ or $5 \\times 5$ window. To compute image’s X and Y gradients, you can use the sobel filter (\\eg ``cv2.Sobel(...)``). For the sum, also think about how you could do it using a convolution filter.\n",
    "\n",
    "You then want to detect corners by finding pixels who’s covariance matrix eigenvalues are large. Since its expensive to compute the eigenvalues explicitly, you should instead compute the response function with:\n",
    "\n",
    "$$R=\\lambda_{1} \\lambda_{2}-k\\left(\\lambda_{1}+\\lambda_{2}\\right)^{2}=\\operatorname{det}(H)-k \\operatorname{tr}(H)^{2}$$\n",
    "\n",
    "$det$ represents the determinant and $tr$ denotes the trace of the matrix. Recall that when detecting corners with the Harris corner detector, corners are points where both eigenvalues are large. This is in contrast to edges (one eigenvalue is larger, while the other is small), and flat regions (both eigenvalues are small). In the response function, the first term becomes very small if one of the eigenvalues are small, thus making $R < 0$. Larger values of $R$ indicates similarly large eigenvalues.\n",
    "\n",
    "Instead of thresholding the response function, simply take the top $\\alpha$ response as the corners, and return their coordinates. A good value for the k parameter is 0.04 - 0.06. <font color=\"blue\">**In your writeup: Show the results of your corner detector on 3 random images from the provided dataset.**</font>\n",
    "\n",
    "**Note: You will be applying this function to a lot of images, try to implement this without loops using vectorization (you should have implemented something similar in HW1). However, there is no penalty if your implementation is slow.**\n",
    "\n",
    "**Another Note: For the next part, you can also try selecting points of interest at random from the images and use it as a baseline method. Compare the two methods (Harris vs Random) and think about why the performance are different or similar. Make sure that your submitted version is using the Harris corner detector though, since we want to make sure that your implementation is sound.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a01b85",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f559288eed41f691e4269bd2ae44bf29",
     "grade": false,
     "grade_id": "cell-61a2b909e40fe7e9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_harris_points(image, alpha, k = 0.05):\n",
    "    '''\n",
    "    Compute points of interest using the Harris corner detector\n",
    "\n",
    "    [input]\n",
    "    * image: numpy.ndarray of shape (H, W) or (H, W, 3)\n",
    "    * alpha: number of points of interest desired\n",
    "    * k: senstivity factor \n",
    "\n",
    "    [output]\n",
    "    * points_of_interest: numpy.ndarray of shape (2, alpha) that contains interest points\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    HINTS:\n",
    "    (1) Visualize and Compare results with cv2.cornerHarris() for debug (DO NOT SUBMIT cv2's implementation)\n",
    "    '''\n",
    "    # ----- TODO -----\n",
    "    \n",
    "    ######### Actual Harris #########\n",
    "    from skimage.color import rgb2gray\n",
    "    from scipy import ndimage\n",
    "\n",
    "\n",
    "    bw_img = rgb2gray(image)\n",
    "    '''\n",
    "    HINTS:\n",
    "    1.> For derivative images we can use cv2.Sobel filter of 3x3 kernel size\n",
    "    2.> For double derivate (e.g. dxx) think of re-using the previous output (e.g. dx)\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    '''\n",
    "    HINTS:\n",
    "    1.> Think of R = det - trace * k\n",
    "    2.> We can use ndimage.convolve\n",
    "    3.> sort (argsort) the values and pick the alpha larges ones\n",
    "    3.> points_of_interest should have this structure [[x1,x2,x3...],[y1,y2,y3...]] (2,alpha)\n",
    "        where x_i is across H and y_i is across W\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    ######### Actual Harris #########\n",
    "    return points_of_interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfbabc0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3f0533549d2222834a8e75cb97e7bec2",
     "grade": false,
     "grade_id": "cell-6021a86b150cf3c8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "path_img = \"./data/aquarium/sun_aztvjgubyrgvirup.jpg\"\n",
    "image = io.imread(path_img)\n",
    "image = image.astype('float')/255\n",
    "\n",
    "alpha = 100\n",
    "\n",
    "harris_points = get_harris_points(image,alpha)\n",
    "plot_harris_points(image, harris_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a323a571",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c6a8bf9c567b5a0fb5611aa856291918",
     "grade": false,
     "grade_id": "cell-9ada7c55263bb3ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<font color=\"blue\">**Show the results of your corner detector on 5 random images from the provided dataset.**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53e7bce",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b3878784d421d3ba37544c3176319b37",
     "grade": false,
     "grade_id": "cell-c03cee2d3ef291cd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q1.2.2 (10 Points Autograder)\n",
    "\n",
    "Now that we have a way to pick interest points from our images, we will build a dictionary using features from these points of interest. You should write the functions:\n",
    "```\n",
    "    def compute_dictionary_one_image(args):\n",
    "  \n",
    "    def compute_dictionary():\n",
    "        \n",
    "```\n",
    "to generate a dictionary given a list of images. The overall goal of ``compute_dictionary()`` is to load the training data, iterate through the paths to the image files to read the images, and extract $\\alpha T$ filter responses over the training files, and call k-means.\n",
    "\n",
    "This can be slow to run; however, the images can be processed independently and in parallel.\n",
    "Inside ``compute_dictionary_one_image()``, you should read an image, extract the responses, and save to a temporary file. Here, ``args`` is a collection of arguments passed into the function.\n",
    "\n",
    "Inside ``compute_dictionary()``, you should load all the training data and create subprocesses to call `` compute_dictionary_one_image()``.\n",
    "\n",
    "After all the subprocesses are finished, load the temporary files back, collect the filter responses, and run k-means.\n",
    "A sensible initial value to try for $K$ is between $100$ and $300$, and for $\\alpha$ is between $50$ and $500$, but they depend on your system configuration and you might want to play with these values.\n",
    "\n",
    "Finally, execute ``compute_dictionary()`` and go get a coffee.\n",
    "If all goes well, you will have a file named ``dictionary.npy`` that contains the dictionary of visual words. \n",
    "If the clustering takes too long, reduce the number of clusters and samples. If you have debugging issues, try passing in a small number of training files manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f928e91",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2894ada43d7c32c27a822dc43670c775",
     "grade": false,
     "grade_id": "cell-74dc3d67ef7a626b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_dictionary_one_image(args):\n",
    "    '''\n",
    "    Extracts samples of the dictionary entries from an image. Use the the \n",
    "    harris corner detector implmented from previous question to extract \n",
    "    the point of interests. This should be a function run by a subprocess.\n",
    "\n",
    "    [input]\n",
    "    * i: index of training image\n",
    "    * alpha: number of samples\n",
    "    * image_path: path of image file\n",
    "\n",
    "    [saved]\n",
    "    * sampled_response: numpy.ndarray of shape (alpha, 3F)\n",
    "    '''\n",
    "    i, alpha, image_path = args\n",
    "    if not os.path.isdir('tmp'):\n",
    "        os.mkdir('tmp')\n",
    "\n",
    "    f_name = 'tmp/%05d.npy' % i\n",
    "    \n",
    "    # ----- TODO -----\n",
    "    '''\n",
    "    HINTS:\n",
    "    1.> Create a tmp dir to store intermediate results.\n",
    "    2.> Read the image from image_path using skimage\n",
    "    3.> extract filter responses and points of interest\n",
    "    4.> store the response of filters at points of interest \n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a73652d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4625d605568eac399ef12d14d4721590",
     "grade": false,
     "grade_id": "cell-4ec3e6b72f002c12",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_dictionary(num_workers=2):\n",
    "    '''\n",
    "    Creates the dictionary of visual words by clustering using k-means.\n",
    "\n",
    "    [input]\n",
    "    * num_workers: number of workers to process in parallel\n",
    "\n",
    "    [saved]\n",
    "    * dictionary: numpy.ndarray of shape (K,3F)\n",
    "    '''\n",
    "\n",
    "    train_data = np.load(\"./data/train_data.npz\")\n",
    "    # ----- TODO -----\n",
    "    list_of_args = []\n",
    "    \n",
    "    '''\n",
    "    Can change these values for experiments, however please submit the dictionary.npy with these values\n",
    "    alpha=150 and n_clusters = 200\n",
    "    '''\n",
    "    alpha = 150\n",
    "    n_clusters = 200\n",
    "\n",
    "    image_names = train_data['files']\n",
    "    num_images = image_names.shape[0]\n",
    "\n",
    "    for i in range(num_images):\n",
    "        full_image_name = './data/' + image_names[i]\n",
    "        list_of_args.append([i, alpha, full_image_name])\n",
    "    \n",
    "    \n",
    "    with multiprocessing.Pool(num_workers) as p:\n",
    "        p.map(compute_dictionary_one_image, list_of_args)\n",
    "    \n",
    "    '''\n",
    "    HINTS:\n",
    "    \n",
    "    1.> Use multiprocessing for parallel processing of elements\n",
    "    2.> Next, load the tmp files and stack the responses stored as npy\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    filter_responses = np.concatenate(filter_responses, axis=0)\n",
    "    \n",
    "    '''\n",
    "    HINTS:\n",
    "    1.> use sklearn.cluster.KMeans for clustersing\n",
    "    2.> dictionary will be the cluster_centers_\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    np.save('dictionary.npy', dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0597cdc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b24fcbcde0c54e356db6de6898571709",
     "grade": false,
     "grade_id": "cell-102de2cb0aaac0eb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**For this assignment, upload a `dictionary.npy` file along with ``alpha = 150`` and ``n_clusters = 200`` for autograding.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30919c1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4528b3a0079956594dd685e38a399130",
     "grade": false,
     "grade_id": "cell-ecbf8d32abca9ad7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q1.3.1 (10 Points -> 5 Autograder + 5 WriteUp)\n",
    "<img align=\"center\" src=\"./figures/textons.jpg\" width=\"800\">\n",
    "<figcaption align = \"center\"><b>Figure 6. Visual words over images. You will use the spatially un-ordered distribution of visual words in a region (a bag of visual words) as a feature for scene classification, with some coarse information provided by spatial pyramid matching [2]</b></figcaption>\n",
    "\n",
    "We want to map each pixel in the image to its closest word in the dictionary.\n",
    "\n",
    "Complete the following function to do this:\n",
    "```\n",
    "                def get_visual_words(image,dictionary):\n",
    "                    return wordmap\n",
    "```\n",
    "and return $wordmap$, a matrix with the same width and height as $image$, where each pixel in $wordmap$ is matched to the closest visual word based on the filter response at the respective pixel in $image$. We will use the standard Euclidean distance to match the filter response and our visual words; to do this efficiently, use the function `` scipy.spatial.distance.cdist()``. Some sample results are shown in Fig. 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10894a7f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7bc7b03e6080cf08301f5e530b8ee152",
     "grade": false,
     "grade_id": "cell-5b648479eec2c96b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_visual_words(image, dictionary):\n",
    "    '''\n",
    "    Compute visual words mapping for the given image using the dictionary of visual words.\n",
    "\n",
    "    [input]\n",
    "    * image: numpy.ndarray of shape (H,W) or (H,W,3)\n",
    "\n",
    "    [output]\n",
    "    * wordmap: numpy.ndarray of shape (H,W)\n",
    "    '''\n",
    "    '''\n",
    "    HINTS:\n",
    "    (1) Use scipy.spatial.distance.cdist to find closest match from dictionary\n",
    "    (2) Wordmap represents the indices of the closest element (np.argmin) in the dictionary\n",
    "    '''\n",
    "    filter_responses = extract_filter_responses(image)\n",
    "    \n",
    "    h, w, _ = filter_responses.shape\n",
    "    filter_responses = np.reshape(filter_responses, [-1, filter_responses.shape[-1]])\n",
    "    # ----- TODO -----\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return wordmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b59c21",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8027352e2b1692db2d5caffa1c9f4bf0",
     "grade": false,
     "grade_id": "cell-849765cd4710de66",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<font color=\"blue\">Visualize three wordmaps of images from any one of the category. **Include these in your write-up, along with the original RGB images. Include some comments on these visualizations: do the “word” boundaries make sense to you?**. We have provided helper function to save and visualize the resulting wordmap in the util.py file. </font> They should look similar to the ones in Figure 6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cb08df",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[1]  James Hays and Alexei A Efros. Scene completion using millions of photographs.ACM Transactions onGraphics (SIGGRAPH 2007), 26(3), 2007.\n",
    "\n",
    "[2]  S. Lazebnik, C. Schmid, and J. Ponce. Beyond bags of features: Spatial pyramid matching for recogniz-ing natural scene categories.  InComputer Vision and Pattern Recognition (CVPR), 2006 IEEE Conferenceon, volume 2, pages 2169–2178, 2006.\n",
    "\n",
    "[3]  Jian xiong Xiao, J. Hays, K. Ehinger, A. Oliva, and A. Torralba. Sun database: Large-scale scene recogni-tion from abbey to zoo.2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition,pages 3485–3492, 2010.14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b40d47",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5902525564dff53bd5383533b6d788a0",
     "grade": false,
     "grade_id": "cell-9ad78ecd8e0d4f38",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Q1.1.2 Manually grade the image\n",
    "\n",
    "Can use `sun_aztvjgubyrgvirup.jpg` for visualizing the 20 image collage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d1f995",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e707846b10ecac92d3d431b4f63c1119",
     "grade": true,
     "grade_id": "cell-168e8e88bb5c301b",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bdf0df",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e79d5a59de481fa9dc828f42526cd52a",
     "grade": false,
     "grade_id": "cell-c6f29e687a1a1485",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Q1.2.1 Manually grade 5 images for Harris Corner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd55c67f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "31ccc7938146b2e47d4aa9ea2f2dcb17",
     "grade": true,
     "grade_id": "cell-75b33723dac9c444",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb762483",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2a5bada2800405e864f0aa54f99094e4",
     "grade": true,
     "grade_id": "q_1_2_1",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04f2c01",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a9fc4f365f489b0195f49c118fa897d1",
     "grade": true,
     "grade_id": "q_1_2_2",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802dc303",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "de7f020c5c5d33d2bbd4200453398867",
     "grade": true,
     "grade_id": "q_1_3_1",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
